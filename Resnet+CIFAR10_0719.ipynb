{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyMPTpMvw3ZO4JhVkP6rLmYU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jundo22l/Jundo22l/blob/main/Resnet%2BCIFAR10_0719.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import AdamW\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# CIFAR-10 데이터셋에 맞춘 데이터 전처리 및 로드\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # w정규화\n",
        "])\n",
        "\n",
        "training_data = CIFAR10(root=\"./\", train=True, download=True, transform=transform)\n",
        "test_data = CIFAR10(root=\"./\", train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
        "\n",
        "# BasicBlock 및 ResNet 모델 정의\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.c1 = nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.c2 = nn.Conv2d(out_channels, out_channels, kernel_size, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.downsample = None\n",
        "        if in_channels != out_channels:\n",
        "            self.downsample = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1)\n",
        "            self.bn_downsample = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_ = x\n",
        "        x = self.c1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.c2(x)\n",
        "        x = self.bn2(x)\n",
        "        if self.downsample is not None:\n",
        "            x_ = self.downsample(x_)\n",
        "            x_ = self.bn_downsample(x_)\n",
        "        x += x_\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.b1 = block(in_channels=3, out_channels=64)\n",
        "        self.b2 = block(in_channels=64, out_channels=128)\n",
        "        self.b3 = block(in_channels=128, out_channels=128)\n",
        "        self.b4 = block(in_channels=128, out_channels=256)\n",
        "        self.b5 = block(in_channels=256, out_channels=256)\n",
        "        self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "        self.fc1 = nn.Linear(256 * 4 * 4, 512)\n",
        "        self.fc2 = nn.Linear(512, num_classes)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.b1(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.b2(x)\n",
        "        x = self.b3(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.b4(x)\n",
        "        x = self.b5(x)\n",
        "        x = self.pool(x)\n",
        "        x = torch.flatten(x, start_dim=1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# 학습 설정\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = ResNet(block=BasicBlock)\n",
        "model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = AdamW(model.parameters(), lr=0.01)  # AdamW 사용\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "# 학습 루프\n",
        "epochs = 15\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    current_lr = scheduler.get_last_lr()[0]\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss 값: {running_loss/len(train_loader)}, Learning Rate: {current_lr}\")\n",
        "\n",
        "# 모델 평가\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"정확도: {100 * correct / total}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eg0-bJhaMe6B",
        "outputId": "2177cf6d-b8dd-4f35-b837-8531d6513adf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:01<00:00, 104809234.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./cifar-10-python.tar.gz to ./\n",
            "Files already downloaded and verified\n",
            "Epoch 1/15, Loss 값: 1.9600951616721385, Learning Rate: 0.01\n",
            "Epoch 2/15, Loss 값: 1.3884017987324453, Learning Rate: 0.01\n",
            "Epoch 3/15, Loss 값: 1.0859399872362767, Learning Rate: 0.01\n",
            "Epoch 4/15, Loss 값: 0.9426564863880577, Learning Rate: 0.01\n",
            "Epoch 5/15, Loss 값: 0.8216971210049241, Learning Rate: 0.01\n",
            "Epoch 6/15, Loss 값: 0.7134205414663495, Learning Rate: 0.01\n",
            "Epoch 7/15, Loss 값: 0.6408171863735789, Learning Rate: 0.01\n",
            "Epoch 8/15, Loss 값: 0.5875648905706528, Learning Rate: 0.01\n",
            "Epoch 9/15, Loss 값: 0.5467912609238759, Learning Rate: 0.01\n",
            "Epoch 10/15, Loss 값: 0.5176796570153492, Learning Rate: 0.001\n",
            "Epoch 11/15, Loss 값: 0.3609920300238425, Learning Rate: 0.001\n",
            "Epoch 12/15, Loss 값: 0.31408481913454395, Learning Rate: 0.001\n",
            "Epoch 13/15, Loss 값: 0.29621379630035144, Learning Rate: 0.001\n",
            "Epoch 14/15, Loss 값: 0.2766284690240917, Learning Rate: 0.001\n",
            "Epoch 15/15, Loss 값: 0.25947721713625105, Learning Rate: 0.001\n",
            "정확도: 87.57%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import AdamW\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# CIFAR-10 데이터셋에 맞춘 데이터 전처리 및 로드\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # w정규화\n",
        "])\n",
        "\n",
        "training_data = CIFAR10(root=\"./\", train=True, download=True, transform=transform)\n",
        "test_data = CIFAR10(root=\"./\", train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
        "\n",
        "# BasicBlock 및 ResNet 모델 정의\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.c1 = nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.c2 = nn.Conv2d(out_channels, out_channels, kernel_size, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.downsample = None\n",
        "        if in_channels != out_channels:\n",
        "            self.downsample = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1)\n",
        "            self.bn_downsample = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_ = x\n",
        "        x = self.c1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.c2(x)\n",
        "        x = self.bn2(x)\n",
        "        if self.downsample is not None:\n",
        "            x_ = self.downsample(x_)\n",
        "            x_ = self.bn_downsample(x_)\n",
        "        x += x_\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.b1 = block(in_channels=3, out_channels=64)\n",
        "        self.b2 = block(in_channels=64, out_channels=128)\n",
        "        self.b3 = block(in_channels=128, out_channels=128)\n",
        "        self.b4 = block(in_channels=128, out_channels=256)\n",
        "        self.b5 = block(in_channels=256, out_channels=256)\n",
        "        self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "        self.fc1 = nn.Linear(256 * 4 * 4, 512)\n",
        "        self.fc2 = nn.Linear(512, num_classes)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.b1(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.b2(x)\n",
        "        x = self.b3(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.b4(x)\n",
        "        x = self.b5(x)\n",
        "        x = self.pool(x)\n",
        "        x = torch.flatten(x, start_dim=1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# 학습 설정\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = ResNet(block=BasicBlock)\n",
        "model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = AdamW(model.parameters(), lr=0.01)  # AdamW 사용\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "# 학습 루프\n",
        "epochs = 30\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    current_lr = scheduler.get_last_lr()[0]\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss 값: {running_loss/len(train_loader)}, Learning Rate: {current_lr}\")\n",
        "\n",
        "# 모델 평가\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"정확도: {100 * correct / total}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKJrxwo-OzrJ",
        "outputId": "febe912f-adef-4f5b-b771-ff0678883ac0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch 1/30, Loss 값: 2.063007611600334, Learning Rate: 0.01\n",
            "Epoch 2/30, Loss 값: 1.4512173114225382, Learning Rate: 0.01\n",
            "Epoch 3/30, Loss 값: 1.2166289658192784, Learning Rate: 0.01\n",
            "Epoch 4/30, Loss 값: 1.0660103633428168, Learning Rate: 0.01\n",
            "Epoch 5/30, Loss 값: 0.9685887392524564, Learning Rate: 0.01\n",
            "Epoch 6/30, Loss 값: 0.8548957917772596, Learning Rate: 0.01\n",
            "Epoch 7/30, Loss 값: 0.7411945664593022, Learning Rate: 0.01\n",
            "Epoch 8/30, Loss 값: 0.6802914263037465, Learning Rate: 0.01\n",
            "Epoch 9/30, Loss 값: 0.62916877736216, Learning Rate: 0.01\n",
            "Epoch 10/30, Loss 값: 0.5914450063158179, Learning Rate: 0.001\n",
            "Epoch 11/30, Loss 값: 0.43143274705580736, Learning Rate: 0.001\n",
            "Epoch 12/30, Loss 값: 0.3921162660621926, Learning Rate: 0.001\n",
            "Epoch 13/30, Loss 값: 0.36950021767822067, Learning Rate: 0.001\n",
            "Epoch 14/30, Loss 값: 0.35707017935602864, Learning Rate: 0.001\n",
            "Epoch 15/30, Loss 값: 0.34339382731929763, Learning Rate: 0.001\n",
            "Epoch 16/30, Loss 값: 0.3286833373253303, Learning Rate: 0.001\n",
            "Epoch 17/30, Loss 값: 0.31492113823171164, Learning Rate: 0.001\n",
            "Epoch 18/30, Loss 값: 0.3055945262122337, Learning Rate: 0.001\n",
            "Epoch 19/30, Loss 값: 0.2939915435336283, Learning Rate: 0.001\n",
            "Epoch 20/30, Loss 값: 0.2850742470711241, Learning Rate: 0.0001\n",
            "Epoch 21/30, Loss 값: 0.2616374597448827, Learning Rate: 0.0001\n",
            "Epoch 22/30, Loss 값: 0.2560770471992395, Learning Rate: 0.0001\n",
            "Epoch 23/30, Loss 값: 0.2506004197194296, Learning Rate: 0.0001\n",
            "Epoch 24/30, Loss 값: 0.24913645330864145, Learning Rate: 0.0001\n",
            "Epoch 25/30, Loss 값: 0.24800973551352615, Learning Rate: 0.0001\n",
            "Epoch 26/30, Loss 값: 0.2479770576290767, Learning Rate: 0.0001\n",
            "Epoch 27/30, Loss 값: 0.24259983783926042, Learning Rate: 0.0001\n",
            "Epoch 28/30, Loss 값: 0.23909776481082828, Learning Rate: 0.0001\n",
            "Epoch 29/30, Loss 값: 0.24246192282384924, Learning Rate: 0.0001\n",
            "Epoch 30/30, Loss 값: 0.23950630270154274, Learning Rate: 1e-05\n",
            "정확도: 87.21%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim import Adam\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# CIFAR-10 데이터셋에 맞춘 데이터 전처리 및 로드\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # 정규화\n",
        "])\n",
        "\n",
        "training_data = CIFAR10(root=\"./\", train=True, download=True, transform=transform)\n",
        "test_data = CIFAR10(root=\"./\", train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
        "\n",
        "# ResidualBlock 및 ResNet 모델 정의\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.stride = stride\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "\n",
        "        self.conv_block = nn.Sequential(\n",
        "            nn.Conv2d(self.in_channels, self.out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(self.out_channels),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Conv2d(self.out_channels, self.out_channels, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(self.out_channels)\n",
        "        )\n",
        "\n",
        "        if self.stride != 1 or self.in_channels != self.out_channels:\n",
        "            self.downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.in_channels, self.out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv_block(x)\n",
        "        if self.stride != 1 or self.in_channels != self.out_channels:\n",
        "            x = self.downsample(x)\n",
        "        out = F.relu(x + out)\n",
        "        return out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_channels = 64\n",
        "        self.base = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.layer1 = self.make_layer(64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self.make_layer(128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self.make_layer(256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self.make_layer(512, num_blocks[3], stride=2)\n",
        "\n",
        "        self.gap = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "    def make_layer(self, out_channels, num_block, stride):\n",
        "        strides = [stride] + [1] * (num_block - 1)\n",
        "        layers = []\n",
        "\n",
        "        for stride in strides:\n",
        "            block = ResidualBlock(self.in_channels, out_channels, stride)\n",
        "            layers.append(block)\n",
        "            self.in_channels = out_channels\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.base(x)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = self.gap(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "def modeltype(model):\n",
        "    if model == 'resnet18':\n",
        "        return ResNet([2, 2, 2, 2])\n",
        "    elif model == 'resnet34':\n",
        "        return ResNet([3, 4, 6, 3])\n",
        "\n",
        "# 학습 설정\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = modeltype('resnet18')\n",
        "model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 학습 루프\n",
        "epochs = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss 값: {running_loss/len(training_data):.4f}\")\n",
        "\n",
        "# 모델 평가\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"정확도: {100 * correct / total:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJ4sLqggbrLb",
        "outputId": "c012b0c5-a330-4a77-ccbe-137d3429ba30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gyYgj51PoeCK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}